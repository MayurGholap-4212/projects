{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMHz6JUVjg6ASYF9ZynWF/z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Excel Analytics Chatbot using Open-Source LLM\n","# Run this in Google Colab for best results\n","\n","# Install required packages\n","!pip install transformers torch gradio pandas openpyxl xlrd sentence-transformers faiss-cpu\n","\n","import pandas as pd\n","import numpy as np\n","import gradio as gr\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","import json\n","import io\n","import base64\n","from typing import List, Dict, Any\n","\n","class ExcelAnalyticsChatbot:\n","    def __init__(self):\n","        # Initialize the open-source LLM\n","        print(\"Loading language model...\")\n","\n","        # Option 1: Use GPT-2 (more reliable for text generation)\n","        model_name = \"gpt2\"\n","\n","        # Option 2: Use a smaller conversational model\n","        # model_name = \"microsoft/DialoGPT-small\"\n","\n","        # Option 3: For code/analytical tasks (uncomment to use)\n","        # model_name = \"Salesforce/codegen-350M-mono\"\n","\n","        try:\n","            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","            # Properly configure padding token\n","            if self.tokenizer.pad_token is None:\n","                if model_name == \"gpt2\":\n","                    self.tokenizer.pad_token = self.tokenizer.eos_token\n","                else:\n","                    self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","                    self.model.resize_token_embeddings(len(self.tokenizer))\n","\n","            # Set model to evaluation mode\n","            self.model.eval()\n","\n","            print(f\"✅ Model {model_name} loaded successfully!\")\n","\n","        except Exception as e:\n","            print(f\"Error loading model: {e}\")\n","            print(\"Falling back to rule-based analysis...\")\n","            self.tokenizer = None\n","            self.model = None\n","\n","        # Initialize sentence transformer for semantic search\n","        print(\"Loading sentence transformer...\")\n","        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","        # Data storage\n","        self.df = None\n","        self.data_summary = None\n","        self.column_info = None\n","        self.embeddings = None\n","        self.faiss_index = None\n","\n","        print(\"Chatbot initialized successfully!\")\n","\n","    def load_excel_file(self, file_path):\n","        \"\"\"Load and analyze Excel/CSV file\"\"\"\n","        try:\n","            # Read file based on extension\n","            file_ext = file_path.lower().split('.')[-1]\n","\n","            if file_ext in ['xlsx', 'xls']:\n","                self.df = pd.read_excel(file_path)\n","                print(f\"✅ Excel file loaded: {self.df.shape}\")\n","\n","            elif file_ext == 'csv':\n","                # Enhanced CSV loading with multiple attempts\n","                print(\"🔄 Attempting to load CSV file...\")\n","\n","                # Try different configurations\n","                csv_configs = [\n","                    {'encoding': 'utf-8', 'sep': ','},\n","                    {'encoding': 'utf-8', 'sep': ';'},\n","                    {'encoding': 'latin-1', 'sep': ','},\n","                    {'encoding': 'cp1252', 'sep': ','},\n","                    {'encoding': 'utf-8', 'sep': '\\t'},\n","                    {'encoding': 'utf-8', 'sep': '|'},\n","                ]\n","\n","                loaded = False\n","                for i, config in enumerate(csv_configs):\n","                    try:\n","                        print(f\"Trying config {i+1}: {config}\")\n","                        self.df = pd.read_csv(file_path, **config)\n","\n","                        # Validate the loaded data\n","                        if (self.df.shape[1] > 1 and\n","                            self.df.shape[0] > 0 and\n","                            not self.df.columns.str.contains(';').any()):  # Check if separator was wrong\n","\n","                            print(f\"✅ CSV loaded successfully with config {i+1}: {self.df.shape}\")\n","                            loaded = True\n","                            break\n","                    except Exception as e:\n","                        print(f\"Config {i+1} failed: {str(e)[:50]}...\")\n","                        continue\n","\n","                if not loaded:\n","                    # Final attempt with pandas auto-detection\n","                    print(\"🔄 Trying pandas auto-detection...\")\n","                    self.df = pd.read_csv(file_path)\n","\n","            else:\n","                return \"❌ Error: Please upload an Excel (.xlsx, .xls) or CSV file.\"\n","\n","            # Validate that we have meaningful data\n","            if self.df.empty:\n","                return \"❌ Error: The uploaded file appears to be empty.\"\n","\n","            if self.df.shape[1] == 1:\n","                return \"⚠️ Warning: Only one column detected. Please check if the CSV separator is correct.\"\n","\n","            # Clean and prepare data\n","            print(\"🔄 Cleaning data...\")\n","\n","            # Clean column names\n","            original_columns = list(self.df.columns)\n","            self.df.columns = self.df.columns.astype(str).str.strip().str.replace('\\n', ' ').str.replace('\\r', ' ')\n","\n","            # Remove completely empty rows and columns\n","            self.df = self.df.dropna(how='all').dropna(axis=1, how='all')\n","\n","            print(f\"📊 Final dataset shape: {self.df.shape}\")\n","            print(f\"📋 Columns: {list(self.df.columns)}\")\n","\n","            # Generate data summary\n","            self.analyze_data()\n","\n","            return f\"\"\"✅ File loaded successfully!\n","\n","**Dataset Information:**\n","- **File Type:** {file_ext.upper()}\n","- **Shape:** {self.df.shape[0]} rows × {self.df.shape[1]} columns\n","- **Columns:** {', '.join(list(self.df.columns)[:5])}{'...' if len(self.df.columns) > 5 else ''}\n","\n","**Sample Data Preview:**\n","{self.df.head(2).to_string()}\n","\n","You can now ask questions about your data! 🚀\"\"\"\n","\n","        except Exception as e:\n","            error_msg = str(e)\n","            return f\"\"\"❌ Error loading file: {error_msg}\n","\n","**Troubleshooting Tips:**\n","- Ensure the file is not corrupted or password-protected\n","- For CSV files, try saving with UTF-8 encoding\n","- Check if the file has proper column headers\n","- Make sure the file contains actual data (not just headers)\n","- Try opening the file in Excel/LibreOffice first to verify it's readable\n","\n","**Supported formats:** .xlsx, .xls, .csv\"\"\"\n","\n","    def analyze_data(self):\n","        \"\"\"Analyze the loaded data and create searchable embeddings\"\"\"\n","        if self.df is None:\n","            return\n","\n","        # Basic data analysis\n","        self.data_summary = {\n","            'shape': self.df.shape,\n","            'columns': list(self.df.columns),\n","            'dtypes': self.df.dtypes.to_dict(),\n","            'null_counts': self.df.isnull().sum().to_dict(),\n","            'numeric_summary': self.df.describe().to_dict() if len(self.df.select_dtypes(include=[np.number]).columns) > 0 else {},\n","            'categorical_summary': {}\n","        }\n","\n","        # Analyze categorical columns\n","        categorical_cols = self.df.select_dtypes(include=['object']).columns\n","        for col in categorical_cols:\n","            unique_values = self.df[col].value_counts().head(10)\n","            self.data_summary['categorical_summary'][col] = unique_values.to_dict()\n","\n","        # Create text representations for semantic search\n","        text_representations = []\n","\n","        # Add column information\n","        for col in self.df.columns:\n","            col_info = f\"Column: {col}, Type: {self.df[col].dtype}\"\n","            if col in self.data_summary['numeric_summary']:\n","                stats = self.data_summary['numeric_summary'][col]\n","                col_info += f\", Mean: {stats.get('mean', 'N/A')}, Max: {stats.get('max', 'N/A')}, Min: {stats.get('min', 'N/A')}\"\n","            text_representations.append(col_info)\n","\n","        # Add sample data representations\n","        for idx, row in self.df.head(10).iterrows():\n","            row_text = f\"Row {idx}: \" + \", \".join([f\"{col}={val}\" for col, val in row.items()])\n","            text_representations.append(row_text)\n","\n","        # Create embeddings\n","        self.embeddings = self.sentence_model.encode(text_representations)\n","\n","        # Create FAISS index for fast similarity search\n","        dimension = self.embeddings.shape[1]\n","        self.faiss_index = faiss.IndexFlatIP(dimension)\n","        self.faiss_index.add(self.embeddings.astype('float32'))\n","\n","        self.text_representations = text_representations\n","\n","    def get_relevant_context(self, query: str, top_k: int = 5) -> str:\n","        \"\"\"Get relevant context from the data based on the query\"\"\"\n","        if self.faiss_index is None:\n","            return \"\"\n","\n","        # Encode query\n","        query_embedding = self.sentence_model.encode([query])\n","\n","        # Search for similar content\n","        scores, indices = self.faiss_index.search(query_embedding.astype('float32'), top_k)\n","\n","        # Get relevant text\n","        relevant_texts = [self.text_representations[idx] for idx in indices[0]]\n","\n","        return \"\\n\".join(relevant_texts)\n","\n","    def generate_insights(self, query: str) -> str:\n","        \"\"\"Generate analytical insights based on the query\"\"\"\n","        if self.df is None:\n","            return \"Please upload an Excel file first.\"\n","\n","        # Get relevant context\n","        context = self.get_relevant_context(query)\n","\n","        # If LLM is not available, use rule-based analysis\n","        if self.model is None or self.tokenizer is None:\n","            return self.generate_data_driven_response(query)\n","\n","        # Create a comprehensive prompt\n","        prompt = f\"\"\"Data Analysis Query:\n","\n","Dataset Information:\n","- Rows: {self.data_summary['shape'][0]}, Columns: {self.data_summary['shape'][1]}\n","- Column Names: {', '.join(self.data_summary['columns'][:5])}{'...' if len(self.data_summary['columns']) > 5 else ''}\n","\n","Context: {context[:200]}...\n","\n","Question: {query}\n","\n","Analysis:\"\"\"\n","\n","        try:\n","            # Generate response using the LLM with proper attention mask\n","            encoded = self.tokenizer(\n","                prompt,\n","                return_tensors='pt',\n","                padding=True,\n","                truncation=True,\n","                max_length=400,  # Reduced for faster processing\n","                return_attention_mask=True\n","            )\n","\n","            input_ids = encoded['input_ids']\n","            attention_mask = encoded['attention_mask']\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    max_length=input_ids.shape[1] + 100,  # Shorter response\n","                    num_return_sequences=1,\n","                    temperature=0.8,\n","                    do_sample=True,\n","                    pad_token_id=self.tokenizer.pad_token_id,\n","                    eos_token_id=self.tokenizer.eos_token_id,\n","                    no_repeat_ngram_size=2,  # Avoid repetition\n","                    early_stopping=True\n","                )\n","\n","            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","            # Extract only the generated part\n","            if \"Analysis:\" in response:\n","                response = response.split(\"Analysis:\")[-1].strip()\n","            else:\n","                response = response[len(prompt):].strip()\n","\n","            # If the response is too short or doesn't make sense, use data-driven approach\n","            if len(response) < 20 or response.count('.') < 1:\n","                return self.generate_data_driven_response(query)\n","\n","            # Combine LLM response with data-driven insights\n","            data_insights = self.generate_data_driven_response(query)\n","            return f\"🤖 **AI Analysis:**\\n{response}\\n\\n📊 **Data-Driven Insights:**\\n{data_insights}\"\n","\n","        except Exception as e:\n","            print(f\"LLM generation error: {e}\")\n","            return self.generate_data_driven_response(query)\n","\n","    def generate_data_driven_response(self, query: str) -> str:\n","        \"\"\"Generate response using direct data analysis\"\"\"\n","        query_lower = query.lower()\n","\n","        # Handle different types of queries\n","        if any(word in query_lower for word in ['summary', 'overview', 'describe']):\n","            return self.get_data_summary()\n","\n","        elif any(word in query_lower for word in ['correlation', 'correlate']):\n","            return self.get_correlation_analysis()\n","\n","        elif any(word in query_lower for word in ['missing', 'null', 'empty']):\n","            return self.get_missing_data_analysis()\n","\n","        elif any(word in query_lower for word in ['distribution', 'histogram']):\n","            return self.get_distribution_analysis()\n","\n","        elif 'trend' in query_lower:\n","            return self.get_trend_analysis()\n","\n","        else:\n","            return self.get_general_insights(query)\n","\n","    def get_data_summary(self) -> str:\n","        \"\"\"Get comprehensive data summary\"\"\"\n","        summary = f\"📊 **Data Summary**\\n\\n\"\n","        summary += f\"**Dataset Shape:** {self.df.shape[0]} rows × {self.df.shape[1]} columns\\n\\n\"\n","\n","        summary += \"**Column Information:**\\n\"\n","        for col, dtype in self.data_summary['dtypes'].items():\n","            null_count = self.data_summary['null_counts'][col]\n","            summary += f\"- {col}: {dtype} ({null_count} missing values)\\n\"\n","\n","        if self.data_summary['numeric_summary']:\n","            summary += \"\\n**Numeric Columns Statistics:**\\n\"\n","            for col, stats in self.data_summary['numeric_summary'].items():\n","                summary += f\"- {col}: Mean={stats.get('mean', 0):.2f}, Std={stats.get('std', 0):.2f}\\n\"\n","\n","        return summary\n","\n","    def get_correlation_analysis(self) -> str:\n","        \"\"\"Analyze correlations between numeric columns\"\"\"\n","        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n","\n","        if len(numeric_cols) < 2:\n","            return \"Not enough numeric columns for correlation analysis.\"\n","\n","        corr_matrix = self.df[numeric_cols].corr()\n","\n","        # Find strongest correlations\n","        correlations = []\n","        for i in range(len(corr_matrix.columns)):\n","            for j in range(i+1, len(corr_matrix.columns)):\n","                corr_val = corr_matrix.iloc[i, j]\n","                if not np.isnan(corr_val):\n","                    correlations.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n","\n","        correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n","\n","        result = \"🔗 **Correlation Analysis**\\n\\n\"\n","        result += \"**Strongest Correlations:**\\n\"\n","\n","        for col1, col2, corr in correlations[:5]:\n","            strength = \"Strong\" if abs(corr) > 0.7 else \"Moderate\" if abs(corr) > 0.3 else \"Weak\"\n","            direction = \"positive\" if corr > 0 else \"negative\"\n","            result += f\"- {col1} ↔ {col2}: {corr:.3f} ({strength} {direction})\\n\"\n","\n","        return result\n","\n","    def get_missing_data_analysis(self) -> str:\n","        \"\"\"Analyze missing data patterns\"\"\"\n","        missing_data = self.df.isnull().sum()\n","        missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n","\n","        if missing_data.empty:\n","            return \"✅ No missing data found in the dataset!\"\n","\n","        result = \"🔍 **Missing Data Analysis**\\n\\n\"\n","        total_rows = len(self.df)\n","\n","        for col, count in missing_data.items():\n","            percentage = (count / total_rows) * 100\n","            result += f\"- {col}: {count} missing ({percentage:.1f}%)\\n\"\n","\n","        return result\n","\n","    def get_distribution_analysis(self) -> str:\n","        \"\"\"Analyze data distributions\"\"\"\n","        result = \"📈 **Distribution Analysis**\\n\\n\"\n","\n","        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n","\n","        for col in numeric_cols[:3]:  # Analyze first 3 numeric columns\n","            series = self.df[col].dropna()\n","            result += f\"**{col}:**\\n\"\n","            result += f\"- Range: {series.min():.2f} to {series.max():.2f}\\n\"\n","            result += f\"- Median: {series.median():.2f}\\n\"\n","            result += f\"- Skewness: {series.skew():.2f}\\n\\n\"\n","\n","        return result\n","\n","    def get_trend_analysis(self) -> str:\n","        \"\"\"Basic trend analysis\"\"\"\n","        result = \"📊 **Trend Analysis**\\n\\n\"\n","\n","        # Look for date columns\n","        date_cols = self.df.select_dtypes(include=['datetime64']).columns\n","\n","        if len(date_cols) == 0:\n","            # Try to find columns that might be dates\n","            potential_date_cols = [col for col in self.df.columns if 'date' in col.lower() or 'time' in col.lower()]\n","            if potential_date_cols:\n","                result += f\"Potential date columns found: {', '.join(potential_date_cols)}\\n\"\n","                result += \"Consider converting these to datetime format for trend analysis.\\n\"\n","            else:\n","                result += \"No date/time columns found for trend analysis.\\n\"\n","        else:\n","            result += f\"Date columns available: {', '.join(date_cols)}\\n\"\n","            result += \"Trend analysis can be performed on time-series data.\\n\"\n","\n","        return result\n","\n","    def get_general_insights(self, query: str) -> str:\n","        \"\"\"Generate general insights based on the query\"\"\"\n","        result = \"💡 **General Insights**\\n\\n\"\n","\n","        # Try to find relevant columns based on query keywords\n","        query_words = query.lower().split()\n","        relevant_cols = []\n","\n","        for word in query_words:\n","            for col in self.df.columns:\n","                if word in col.lower():\n","                    relevant_cols.append(col)\n","\n","        if relevant_cols:\n","            result += f\"Found relevant columns: {', '.join(set(relevant_cols))}\\n\\n\"\n","\n","            for col in set(relevant_cols):\n","                if self.df[col].dtype in ['int64', 'float64']:\n","                    result += f\"**{col}:** Mean = {self.df[col].mean():.2f}, Std = {self.df[col].std():.2f}\\n\"\n","                else:\n","                    top_values = self.df[col].value_counts().head(3)\n","                    result += f\"**{col}:** Top values = {dict(top_values)}\\n\"\n","        else:\n","            result += \"I'd be happy to help analyze your data! Try asking about:\\n\"\n","            result += \"- Data summary or overview\\n\"\n","            result += \"- Correlations between columns\\n\"\n","            result += \"- Missing data analysis\\n\"\n","            result += \"- Distribution of specific columns\\n\"\n","\n","        return result\n","\n","# Initialize the chatbot\n","chatbot = ExcelAnalyticsChatbot()\n","\n","def process_file_and_query(file, query):\n","    \"\"\"Process uploaded file and answer query\"\"\"\n","    if file is not None:\n","        try:\n","            # Get file path and extension\n","            file_path = file.name\n","            file_ext = file_path.lower().split('.')[-1]\n","\n","            # Debug info\n","            print(f\"Processing file: {file_path}, Extension: {file_ext}\")\n","\n","            # Load the file\n","            load_result = chatbot.load_excel_file(file_path)\n","\n","            if query.strip():\n","                # Answer the query\n","                response = chatbot.generate_insights(query)\n","                return f\"{load_result}\\n\\n---\\n\\n**Your Question:** {query}\\n\\n**Answer:**\\n{response}\"\n","            else:\n","                return load_result\n","\n","        except Exception as e:\n","            return f\"Error processing file: {str(e)}\\n\\nPlease make sure you've uploaded a valid Excel (.xlsx, .xls) or CSV file.\"\n","    else:\n","        if query.strip():\n","            return chatbot.generate_insights(query)\n","        else:\n","            return \"Please upload an Excel/CSV file and/or ask a question about your data.\"\n","\n","# Create Gradio interface\n","def create_interface():\n","    with gr.Blocks(title=\"Excel Analytics Chatbot\", theme=gr.themes.Soft()) as interface:\n","        gr.Markdown(\"\"\"\n","        # 📊 Excel Analytics Chatbot\n","\n","        Upload your Excel/CSV file and ask questions about your data! This chatbot uses open-source LLMs to provide analytical insights.\n","\n","        **Supported File Types:** .xlsx, .xls, .csv\n","\n","        **Example Questions:**\n","        - \"Give me a summary of this data\"\n","        - \"What are the correlations between columns?\"\n","        - \"Which columns have missing data?\"\n","        - \"Show me the distribution of [column name]\"\n","        - \"What trends can you identify?\"\n","\n","        **CSV Troubleshooting:** If your CSV doesn't load properly, make sure it's saved with UTF-8 encoding and uses comma separators.\n","        \"\"\")\n","\n","        with gr.Row():\n","            with gr.Column(scale=1):\n","                file_upload = gr.File(\n","                    label=\"📁 Upload Excel/CSV File\",\n","                    file_types=[\".xlsx\", \".xls\", \".csv\"]\n","                )\n","\n","                # Add a test CSV button\n","                test_csv_btn = gr.Button(\"🧪 Test with Sample CSV\", variant=\"secondary\", size=\"sm\")\n","\n","            with gr.Column(scale=2):\n","                query_input = gr.Textbox(\n","                    label=\"❓ Ask a question about your data\",\n","                    placeholder=\"e.g., 'What are the main patterns in this data?'\",\n","                    lines=3\n","                )\n","\n","        submit_btn = gr.Button(\"🔍 Analyze Data\", variant=\"primary\", size=\"lg\")\n","\n","        output = gr.Textbox(\n","            label=\"📋 Analysis Results\",\n","            lines=15,\n","            max_lines=30\n","        )\n","\n","        # Function to create and test with sample CSV\n","        def create_test_csv():\n","            import tempfile\n","            import os\n","\n","            # Create sample data\n","            sample_data = {\n","                'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Tablet'],\n","                'Price': [999.99, 25.50, 75.00, 299.99, 449.99],\n","                'Sales': [150, 500, 300, 120, 200],\n","                'Category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Electronics'],\n","                'Rating': [4.5, 4.2, 4.0, 4.8, 4.3]\n","            }\n","\n","            df = pd.DataFrame(sample_data)\n","\n","            # Create temporary file\n","            temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n","            df.to_csv(temp_file.name, index=False)\n","            temp_file.close()\n","\n","            return temp_file.name\n","\n","        def test_with_sample_csv():\n","            try:\n","                csv_file = create_test_csv()\n","                result = chatbot.load_excel_file(csv_file)\n","\n","                # Clean up\n","                import os\n","                os.unlink(csv_file)\n","\n","                return result + \"\\n\\n🎯 **Sample loaded successfully!** You can now test queries like 'What is the average price?' or 'Show correlation between price and sales'.\"\n","            except Exception as e:\n","                return f\"Error creating sample CSV: {str(e)}\"\n","\n","        # Examples\n","        gr.Examples(\n","            examples=[\n","                [None, \"Give me a summary of the data\"],\n","                [None, \"What correlations exist between numeric columns?\"],\n","                [None, \"Which columns have missing values?\"],\n","                [None, \"Analyze the distribution of the data\"],\n","                [None, \"What insights can you provide about this dataset?\"],\n","                [None, \"Show me statistics for all numeric columns\"],\n","                [None, \"What are the unique values in categorical columns?\"]\n","            ],\n","            inputs=[file_upload, query_input]\n","        )\n","\n","        submit_btn.click(\n","            fn=process_file_and_query,\n","            inputs=[file_upload, query_input],\n","            outputs=output\n","        )\n","\n","        test_csv_btn.click(\n","            fn=test_with_sample_csv,\n","            outputs=output\n","        )\n","\n","    return interface\n","\n","# Launch the interface\n","if __name__ == \"__main__\":\n","    interface = create_interface()\n","    interface.launch(share=True, debug=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BBbMiap9Ksap","executionInfo":{"status":"ok","timestamp":1749818673565,"user_tz":-330,"elapsed":737502,"user":{"displayName":"Mayur Gholap","userId":"11537871301684619206"}},"outputId":"2326e185-0fc6-49d9-b2dd-267b3dba1441"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n","Requirement already satisfied: xlrd in /usr/local/lib/python3.11/dist-packages (2.0.1)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n","Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n","Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n","Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n","Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n","Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Loading language model...\n","✅ Model gpt2 loaded successfully!\n","Loading sentence transformer...\n","Chatbot initialized successfully!\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://e001e15c71e35a803d.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://e001e15c71e35a803d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /tmp/gradio/f1ac34adc9959754c44f14f901416071eef3497736a176c94d0d91441ea885ca/sales_data.csv, Extension: csv\n","🔄 Attempting to load CSV file...\n","Trying config 1: {'encoding': 'utf-8', 'sep': ','}\n","✅ CSV loaded successfully with config 1: (4, 13)\n","🔄 Cleaning data...\n","📊 Final dataset shape: (4, 13)\n","📋 Columns: ['1', '2024-01-01', 'Alice Shah', 'alice@example.com', '9876543210', 'Laptop', 'Electronics', '1.1', '60000.0', '60000.0.1', 'Credit Card', 'Mumbai, MH', 'Delivered']\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /tmp/gradio/e1dfc3e1f9f864339749d73d86f43c45bde46dbe35f2c47385415bc9bcab4f0d/sales_data_sample copy.csv, Extension: csv\n","🔄 Attempting to load CSV file...\n","Trying config 1: {'encoding': 'utf-8', 'sep': ','}\n","Config 1 failed: 'utf-8' codec can't decode byte 0x84 in position 8...\n","Trying config 2: {'encoding': 'utf-8', 'sep': ';'}\n","Config 2 failed: 'utf-8' codec can't decode byte 0x84 in position 1...\n","Trying config 3: {'encoding': 'latin-1', 'sep': ','}\n","✅ CSV loaded successfully with config 3: (2823, 25)\n","🔄 Cleaning data...\n","📊 Final dataset shape: (2823, 25)\n","📋 Columns: ['ORDERNUMBER', 'QUANTITYORDERED', 'PRICEEACH', 'ORDERLINENUMBER', 'SALES', 'ORDERDATE', 'STATUS', 'QTR_ID', 'MONTH_ID', 'YEAR_ID', 'PRODUCTLINE', 'MSRP', 'PRODUCTCODE', 'CUSTOMERNAME', 'PHONE', 'ADDRESSLINE1', 'ADDRESSLINE2', 'CITY', 'STATE', 'POSTALCODE', 'COUNTRY', 'TERRITORY', 'CONTACTLASTNAME', 'CONTACTFIRSTNAME', 'DEALSIZE']\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /tmp/gradio/e1dfc3e1f9f864339749d73d86f43c45bde46dbe35f2c47385415bc9bcab4f0d/sales_data_sample copy.csv, Extension: csv\n","🔄 Attempting to load CSV file...\n","Trying config 1: {'encoding': 'utf-8', 'sep': ','}\n","Config 1 failed: 'utf-8' codec can't decode byte 0x84 in position 8...\n","Trying config 2: {'encoding': 'utf-8', 'sep': ';'}\n","Config 2 failed: 'utf-8' codec can't decode byte 0x84 in position 1...\n","Trying config 3: {'encoding': 'latin-1', 'sep': ','}\n","✅ CSV loaded successfully with config 3: (2823, 25)\n","🔄 Cleaning data...\n","📊 Final dataset shape: (2823, 25)\n","📋 Columns: ['ORDERNUMBER', 'QUANTITYORDERED', 'PRICEEACH', 'ORDERLINENUMBER', 'SALES', 'ORDERDATE', 'STATUS', 'QTR_ID', 'MONTH_ID', 'YEAR_ID', 'PRODUCTLINE', 'MSRP', 'PRODUCTCODE', 'CUSTOMERNAME', 'PHONE', 'ADDRESSLINE1', 'ADDRESSLINE2', 'CITY', 'STATE', 'POSTALCODE', 'COUNTRY', 'TERRITORY', 'CONTACTLASTNAME', 'CONTACTFIRSTNAME', 'DEALSIZE']\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /tmp/gradio/e1dfc3e1f9f864339749d73d86f43c45bde46dbe35f2c47385415bc9bcab4f0d/sales_data_sample copy.csv, Extension: csv\n","🔄 Attempting to load CSV file...\n","Trying config 1: {'encoding': 'utf-8', 'sep': ','}\n","Config 1 failed: 'utf-8' codec can't decode byte 0x84 in position 8...\n","Trying config 2: {'encoding': 'utf-8', 'sep': ';'}\n","Config 2 failed: 'utf-8' codec can't decode byte 0x84 in position 1...\n","Trying config 3: {'encoding': 'latin-1', 'sep': ','}\n","✅ CSV loaded successfully with config 3: (2823, 25)\n","🔄 Cleaning data...\n","📊 Final dataset shape: (2823, 25)\n","📋 Columns: ['ORDERNUMBER', 'QUANTITYORDERED', 'PRICEEACH', 'ORDERLINENUMBER', 'SALES', 'ORDERDATE', 'STATUS', 'QTR_ID', 'MONTH_ID', 'YEAR_ID', 'PRODUCTLINE', 'MSRP', 'PRODUCTCODE', 'CUSTOMERNAME', 'PHONE', 'ADDRESSLINE1', 'ADDRESSLINE2', 'CITY', 'STATE', 'POSTALCODE', 'COUNTRY', 'TERRITORY', 'CONTACTLASTNAME', 'CONTACTFIRSTNAME', 'DEALSIZE']\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /tmp/gradio/e1dfc3e1f9f864339749d73d86f43c45bde46dbe35f2c47385415bc9bcab4f0d/sales_data_sample copy.csv, Extension: csv\n","🔄 Attempting to load CSV file...\n","Trying config 1: {'encoding': 'utf-8', 'sep': ','}\n","Config 1 failed: 'utf-8' codec can't decode byte 0x84 in position 8...\n","Trying config 2: {'encoding': 'utf-8', 'sep': ';'}\n","Config 2 failed: 'utf-8' codec can't decode byte 0x84 in position 1...\n","Trying config 3: {'encoding': 'latin-1', 'sep': ','}\n","✅ CSV loaded successfully with config 3: (2823, 25)\n","🔄 Cleaning data...\n","📊 Final dataset shape: (2823, 25)\n","📋 Columns: ['ORDERNUMBER', 'QUANTITYORDERED', 'PRICEEACH', 'ORDERLINENUMBER', 'SALES', 'ORDERDATE', 'STATUS', 'QTR_ID', 'MONTH_ID', 'YEAR_ID', 'PRODUCTLINE', 'MSRP', 'PRODUCTCODE', 'CUSTOMERNAME', 'PHONE', 'ADDRESSLINE1', 'ADDRESSLINE2', 'CITY', 'STATE', 'POSTALCODE', 'COUNTRY', 'TERRITORY', 'CONTACTLASTNAME', 'CONTACTFIRSTNAME', 'DEALSIZE']\n","Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://e001e15c71e35a803d.gradio.live\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Gbe6qPmjOtyf"},"execution_count":null,"outputs":[]}]}