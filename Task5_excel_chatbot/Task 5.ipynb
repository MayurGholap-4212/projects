{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMHz6JUVjg6ASYF9ZynWF/z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Excel Analytics Chatbot using Open-Source LLM\n","# Run this in Google Colab for best results\n","\n","# Install required packages\n","!pip install transformers torch gradio pandas openpyxl xlrd sentence-transformers faiss-cpu\n","\n","import pandas as pd\n","import numpy as np\n","import gradio as gr\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","import json\n","import io\n","import base64\n","from typing import List, Dict, Any\n","\n","class ExcelAnalyticsChatbot:\n","    def __init__(self):\n","        # Initialize the open-source LLM\n","        print(\"Loading language model...\")\n","\n","        # Option 1: Use GPT-2 (more reliable for text generation)\n","        model_name = \"gpt2\"\n","\n","        # Option 2: Use a smaller conversational model\n","        # model_name = \"microsoft/DialoGPT-small\"\n","\n","        # Option 3: For code/analytical tasks (uncomment to use)\n","        # model_name = \"Salesforce/codegen-350M-mono\"\n","\n","        try:\n","            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","            self.model = AutoModelForCausalLM.from_pretrained(model_name)\n","\n","            # Properly configure padding token\n","            if self.tokenizer.pad_token is None:\n","                if model_name == \"gpt2\":\n","                    self.tokenizer.pad_token = self.tokenizer.eos_token\n","                else:\n","                    self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","                    self.model.resize_token_embeddings(len(self.tokenizer))\n","\n","            # Set model to evaluation mode\n","            self.model.eval()\n","\n","            print(f\"‚úÖ Model {model_name} loaded successfully!\")\n","\n","        except Exception as e:\n","            print(f\"Error loading model: {e}\")\n","            print(\"Falling back to rule-based analysis...\")\n","            self.tokenizer = None\n","            self.model = None\n","\n","        # Initialize sentence transformer for semantic search\n","        print(\"Loading sentence transformer...\")\n","        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","        # Data storage\n","        self.df = None\n","        self.data_summary = None\n","        self.column_info = None\n","        self.embeddings = None\n","        self.faiss_index = None\n","\n","        print(\"Chatbot initialized successfully!\")\n","\n","    def load_excel_file(self, file_path):\n","        \"\"\"Load and analyze Excel/CSV file\"\"\"\n","        try:\n","            # Read file based on extension\n","            file_ext = file_path.lower().split('.')[-1]\n","\n","            if file_ext in ['xlsx', 'xls']:\n","                self.df = pd.read_excel(file_path)\n","                print(f\"‚úÖ Excel file loaded: {self.df.shape}\")\n","\n","            elif file_ext == 'csv':\n","                # Enhanced CSV loading with multiple attempts\n","                print(\"üîÑ Attempting to load CSV file...\")\n","\n","                # Try different configurations\n","                csv_configs = [\n","                    {'encoding': 'utf-8', 'sep': ','},\n","                    {'encoding': 'utf-8', 'sep': ';'},\n","                    {'encoding': 'latin-1', 'sep': ','},\n","                    {'encoding': 'cp1252', 'sep': ','},\n","                    {'encoding': 'utf-8', 'sep': '\\t'},\n","                    {'encoding': 'utf-8', 'sep': '|'},\n","                ]\n","\n","                loaded = False\n","                for i, config in enumerate(csv_configs):\n","                    try:\n","                        print(f\"Trying config {i+1}: {config}\")\n","                        self.df = pd.read_csv(file_path, **config)\n","\n","                        # Validate the loaded data\n","                        if (self.df.shape[1] > 1 and\n","                            self.df.shape[0] > 0 and\n","                            not self.df.columns.str.contains(';').any()):  # Check if separator was wrong\n","\n","                            print(f\"‚úÖ CSV loaded successfully with config {i+1}: {self.df.shape}\")\n","                            loaded = True\n","                            break\n","                    except Exception as e:\n","                        print(f\"Config {i+1} failed: {str(e)[:50]}...\")\n","                        continue\n","\n","                if not loaded:\n","                    # Final attempt with pandas auto-detection\n","                    print(\"üîÑ Trying pandas auto-detection...\")\n","                    self.df = pd.read_csv(file_path)\n","\n","            else:\n","                return \"‚ùå Error: Please upload an Excel (.xlsx, .xls) or CSV file.\"\n","\n","            # Validate that we have meaningful data\n","            if self.df.empty:\n","                return \"‚ùå Error: The uploaded file appears to be empty.\"\n","\n","            if self.df.shape[1] == 1:\n","                return \"‚ö†Ô∏è Warning: Only one column detected. Please check if the CSV separator is correct.\"\n","\n","            # Clean and prepare data\n","            print(\"üîÑ Cleaning data...\")\n","\n","            # Clean column names\n","            original_columns = list(self.df.columns)\n","            self.df.columns = self.df.columns.astype(str).str.strip().str.replace('\\n', ' ').str.replace('\\r', ' ')\n","\n","            # Remove completely empty rows and columns\n","            self.df = self.df.dropna(how='all').dropna(axis=1, how='all')\n","\n","            print(f\"üìä Final dataset shape: {self.df.shape}\")\n","            print(f\"üìã Columns: {list(self.df.columns)}\")\n","\n","            # Generate data summary\n","            self.analyze_data()\n","\n","            return f\"\"\"‚úÖ File loaded successfully!\n","\n","**Dataset Information:**\n","- **File Type:** {file_ext.upper()}\n","- **Shape:** {self.df.shape[0]} rows √ó {self.df.shape[1]} columns\n","- **Columns:** {', '.join(list(self.df.columns)[:5])}{'...' if len(self.df.columns) > 5 else ''}\n","\n","**Sample Data Preview:**\n","{self.df.head(2).to_string()}\n","\n","You can now ask questions about your data! üöÄ\"\"\"\n","\n","        except Exception as e:\n","            error_msg = str(e)\n","            return f\"\"\"‚ùå Error loading file: {error_msg}\n","\n","**Troubleshooting Tips:**\n","- Ensure the file is not corrupted or password-protected\n","- For CSV files, try saving with UTF-8 encoding\n","- Check if the file has proper column headers\n","- Make sure the file contains actual data (not just headers)\n","- Try opening the file in Excel/LibreOffice first to verify it's readable\n","\n","**Supported formats:** .xlsx, .xls, .csv\"\"\"\n","\n","    def analyze_data(self):\n","        \"\"\"Analyze the loaded data and create searchable embeddings\"\"\"\n","        if self.df is None:\n","            return\n","\n","        # Basic data analysis\n","        self.data_summary = {\n","            'shape': self.df.shape,\n","            'columns': list(self.df.columns),\n","            'dtypes': self.df.dtypes.to_dict(),\n","            'null_counts': self.df.isnull().sum().to_dict(),\n","            'numeric_summary': self.df.describe().to_dict() if len(self.df.select_dtypes(include=[np.number]).columns) > 0 else {},\n","            'categorical_summary': {}\n","        }\n","\n","        # Analyze categorical columns\n","        categorical_cols = self.df.select_dtypes(include=['object']).columns\n","        for col in categorical_cols:\n","            unique_values = self.df[col].value_counts().head(10)\n","            self.data_summary['categorical_summary'][col] = unique_values.to_dict()\n","\n","        # Create text representations for semantic search\n","        text_representations = []\n","\n","        # Add column information\n","        for col in self.df.columns:\n","            col_info = f\"Column: {col}, Type: {self.df[col].dtype}\"\n","            if col in self.data_summary['numeric_summary']:\n","                stats = self.data_summary['numeric_summary'][col]\n","                col_info += f\", Mean: {stats.get('mean', 'N/A')}, Max: {stats.get('max', 'N/A')}, Min: {stats.get('min', 'N/A')}\"\n","            text_representations.append(col_info)\n","\n","        # Add sample data representations\n","        for idx, row in self.df.head(10).iterrows():\n","            row_text = f\"Row {idx}: \" + \", \".join([f\"{col}={val}\" for col, val in row.items()])\n","            text_representations.append(row_text)\n","\n","        # Create embeddings\n","        self.embeddings = self.sentence_model.encode(text_representations)\n","\n","        # Create FAISS index for fast similarity search\n","        dimension = self.embeddings.shape[1]\n","        self.faiss_index = faiss.IndexFlatIP(dimension)\n","        self.faiss_index.add(self.embeddings.astype('float32'))\n","\n","        self.text_representations = text_representations\n","\n","    def get_relevant_context(self, query: str, top_k: int = 5) -> str:\n","        \"\"\"Get relevant context from the data based on the query\"\"\"\n","        if self.faiss_index is None:\n","            return \"\"\n","\n","        # Encode query\n","        query_embedding = self.sentence_model.encode([query])\n","\n","        # Search for similar content\n","        scores, indices = self.faiss_index.search(query_embedding.astype('float32'), top_k)\n","\n","        # Get relevant text\n","        relevant_texts = [self.text_representations[idx] for idx in indices[0]]\n","\n","        return \"\\n\".join(relevant_texts)\n","\n","    def generate_insights(self, query: str) -> str:\n","        \"\"\"Generate analytical insights based on the query\"\"\"\n","        if self.df is None:\n","            return \"Please upload an Excel file first.\"\n","\n","        # Get relevant context\n","        context = self.get_relevant_context(query)\n","\n","        # If LLM is not available, use rule-based analysis\n","        if self.model is None or self.tokenizer is None:\n","            return self.generate_data_driven_response(query)\n","\n","        # Create a comprehensive prompt\n","        prompt = f\"\"\"Data Analysis Query:\n","\n","Dataset Information:\n","- Rows: {self.data_summary['shape'][0]}, Columns: {self.data_summary['shape'][1]}\n","- Column Names: {', '.join(self.data_summary['columns'][:5])}{'...' if len(self.data_summary['columns']) > 5 else ''}\n","\n","Context: {context[:200]}...\n","\n","Question: {query}\n","\n","Analysis:\"\"\"\n","\n","        try:\n","            # Generate response using the LLM with proper attention mask\n","            encoded = self.tokenizer(\n","                prompt,\n","                return_tensors='pt',\n","                padding=True,\n","                truncation=True,\n","                max_length=400,  # Reduced for faster processing\n","                return_attention_mask=True\n","            )\n","\n","            input_ids = encoded['input_ids']\n","            attention_mask = encoded['attention_mask']\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    max_length=input_ids.shape[1] + 100,  # Shorter response\n","                    num_return_sequences=1,\n","                    temperature=0.8,\n","                    do_sample=True,\n","                    pad_token_id=self.tokenizer.pad_token_id,\n","                    eos_token_id=self.tokenizer.eos_token_id,\n","                    no_repeat_ngram_size=2,  # Avoid repetition\n","                    early_stopping=True\n","                )\n","\n","            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","            # Extract only the generated part\n","            if \"Analysis:\" in response:\n","                response = response.split(\"Analysis:\")[-1].strip()\n","            else:\n","                response = response[len(prompt):].strip()\n","\n","            # If the response is too short or doesn't make sense, use data-driven approach\n","            if len(response) < 20 or response.count('.') < 1:\n","                return self.generate_data_driven_response(query)\n","\n","            # Combine LLM response with data-driven insights\n","            data_insights = self.generate_data_driven_response(query)\n","            return f\"ü§ñ **AI Analysis:**\\n{response}\\n\\nüìä **Data-Driven Insights:**\\n{data_insights}\"\n","\n","        except Exception as e:\n","            print(f\"LLM generation error: {e}\")\n","            return self.generate_data_driven_response(query)\n","\n","    def generate_data_driven_response(self, query: str) -> str:\n","        \"\"\"Generate response using direct data analysis\"\"\"\n","        query_lower = query.lower()\n","\n","        # Handle different types of queries\n","        if any(word in query_lower for word in ['summary', 'overview', 'describe']):\n","            return self.get_data_summary()\n","\n","        elif any(word in query_lower for word in ['correlation', 'correlate']):\n","            return self.get_correlation_analysis()\n","\n","        elif any(word in query_lower for word in ['missing', 'null', 'empty']):\n","            return self.get_missing_data_analysis()\n","\n","        elif any(word in query_lower for word in ['distribution', 'histogram']):\n","            return self.get_distribution_analysis()\n","\n","        elif 'trend' in query_lower:\n","            return self.get_trend_analysis()\n","\n","        else:\n","            return self.get_general_insights(query)\n","\n","    def get_data_summary(self) -> str:\n","        \"\"\"Get comprehensive data summary\"\"\"\n","        summary = f\"üìä **Data Summary**\\n\\n\"\n","        summary += f\"**Dataset Shape:** {self.df.shape[0]} rows √ó {self.df.shape[1]} columns\\n\\n\"\n","\n","        summary += \"**Column Information:**\\n\"\n","        for col, dtype in self.data_summary['dtypes'].items():\n","            null_count = self.data_summary['null_counts'][col]\n","            summary += f\"- {col}: {dtype} ({null_count} missing values)\\n\"\n","\n","        if self.data_summary['numeric_summary']:\n","            summary += \"\\n**Numeric Columns Statistics:**\\n\"\n","            for col, stats in self.data_summary['numeric_summary'].items():\n","                summary += f\"- {col}: Mean={stats.get('mean', 0):.2f}, Std={stats.get('std', 0):.2f}\\n\"\n","\n","        return summary\n","\n","    def get_correlation_analysis(self) -> str:\n","        \"\"\"Analyze correlations between numeric columns\"\"\"\n","        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n","\n","        if len(numeric_cols) < 2:\n","            return \"Not enough numeric columns for correlation analysis.\"\n","\n","        corr_matrix = self.df[numeric_cols].corr()\n","\n","        # Find strongest correlations\n","        correlations = []\n","        for i in range(len(corr_matrix.columns)):\n","            for j in range(i+1, len(corr_matrix.columns)):\n","                corr_val = corr_matrix.iloc[i, j]\n","                if not np.isnan(corr_val):\n","                    correlations.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))\n","\n","        correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n","\n","        result = \"üîó **Correlation Analysis**\\n\\n\"\n","        result += \"**Strongest Correlations:**\\n\"\n","\n","        for col1, col2, corr in correlations[:5]:\n","            strength = \"Strong\" if abs(corr) > 0.7 else \"Moderate\" if abs(corr) > 0.3 else \"Weak\"\n","            direction = \"positive\" if corr > 0 else \"negative\"\n","            result += f\"- {col1} ‚Üî {col2}: {corr:.3f} ({strength} {direction})\\n\"\n","\n","        return result\n","\n","    def get_missing_data_analysis(self) -> str:\n","        \"\"\"Analyze missing data patterns\"\"\"\n","        missing_data = self.df.isnull().sum()\n","        missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n","\n","        if missing_data.empty:\n","            return \"‚úÖ No missing data found in the dataset!\"\n","\n","        result = \"üîç **Missing Data Analysis**\\n\\n\"\n","        total_rows = len(self.df)\n","\n","        for col, count in missing_data.items():\n","            percentage = (count / total_rows) * 100\n","            result += f\"- {col}: {count} missing ({percentage:.1f}%)\\n\"\n","\n","        return result\n","\n","    def get_distribution_analysis(self) -> str:\n","        \"\"\"Analyze data distributions\"\"\"\n","        result = \"üìà **Distribution Analysis**\\n\\n\"\n","\n","        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n","\n","        for col in numeric_cols[:3]:  # Analyze first 3 numeric columns\n","            series = self.df[col].dropna()\n","            result += f\"**{col}:**\\n\"\n","            result += f\"- Range: {series.min():.2f} to {series.max():.2f}\\n\"\n","            result += f\"- Median: {series.median():.2f}\\n\"\n","            result += f\"- Skewness: {series.skew():.2f}\\n\\n\"\n","\n","        return result\n","\n","    def get_trend_analysis(self) -> str:\n","        \"\"\"Basic trend analysis\"\"\"\n","        result = \"üìä **Trend Analysis**\\n\\n\"\n","\n","        # Look for date columns\n","        date_cols = self.df.select_dtypes(include=['datetime64']).columns\n","\n","        if len(date_cols) == 0:\n","            # Try to find columns that might be dates\n","            potential_date_cols = [col for col in self.df.columns if 'date' in col.lower() or 'time' in col.lower()]\n","            if potential_date_cols:\n","                result += f\"Potential date columns found: {', '.join(potential_date_cols)}\\n\"\n","                result += \"Consider converting these to datetime format for trend analysis.\\n\"\n","            else:\n","                result += \"No date/time columns found for trend analysis.\\n\"\n","        else:\n","            result += f\"Date columns available: {', '.join(date_cols)}\\n\"\n","            result += \"Trend analysis can be performed on time-series data.\\n\"\n","\n","        return result\n","\n","    def get_general_insights(self, query: str) -> str:\n","        \"\"\"Generate general insights based on the query\"\"\"\n","        result = \"üí° **General Insights**\\n\\n\"\n","\n","        # Try to find relevant columns based on query keywords\n","        query_words = query.lower().split()\n","        relevant_cols = []\n","\n","        for word in query_words:\n","            for col in self.df.columns:\n","                if word in col.lower():\n","                    relevant_cols.append(col)\n","\n","        if relevant_cols:\n","            result += f\"Found relevant columns: {', '.join(set(relevant_cols))}\\n\\n\"\n","\n","            for col in set(relevant_cols):\n","                if self.df[col].dtype in ['int64', 'float64']:\n","                    result += f\"**{col}:** Mean = {self.df[col].mean():.2f}, Std = {self.df[col].std():.2f}\\n\"\n","                else:\n","                    top_values = self.df[col].value_counts().head(3)\n","                    result += f\"**{col}:** Top values = {dict(top_values)}\\n\"\n","        else:\n","            result += \"I'd be happy to help analyze your data! Try asking about:\\n\"\n","            result += \"- Data summary or overview\\n\"\n","            result += \"- Correlations between columns\\n\"\n","            result += \"- Missing data analysis\\n\"\n","            result += \"- Distribution of specific columns\\n\"\n","\n","        return result\n","\n","# Initialize the chatbot\n","chatbot = ExcelAnalyticsChatbot()\n","\n","def process_file_and_query(file, query):\n","    \"\"\"Process uploaded file and answer query\"\"\"\n","    if file is not None:\n","        try:\n","            # Get file path and extension\n","            file_path = file.name\n","            file_ext = file_path.lower().split('.')[-1]\n","\n","            # Debug info\n","            print(f\"Processing file: {file_path}, Extension: {file_ext}\")\n","\n","            # Load the file\n","            load_result = chatbot.load_excel_file(file_path)\n","\n","            if query.strip():\n","                # Answer the query\n","                response = chatbot.generate_insights(query)\n","                return f\"{load_result}\\n\\n---\\n\\n**Your Question:** {query}\\n\\n**Answer:**\\n{response}\"\n","            else:\n","                return load_result\n","\n","        except Exception as e:\n","            return f\"Error processing file: {str(e)}\\n\\nPlease make sure you've uploaded a valid Excel (.xlsx, .xls) or CSV file.\"\n","    else:\n","        if query.strip():\n","            return chatbot.generate_insights(query)\n","        else:\n","            return \"Please upload an Excel/CSV file and/or ask a question about your data.\"\n","\n","# Create Gradio interface\n","def create_interface():\n","    with gr.Blocks(title=\"Excel Analytics Chatbot\", theme=gr.themes.Soft()) as interface:\n","        gr.Markdown(\"\"\"\n","        # üìä Excel Analytics Chatbot\n","\n","        Upload your Excel/CSV file and ask questions about your data! This chatbot uses open-source LLMs to provide analytical insights.\n","\n","        **Supported File Types:** .xlsx, .xls, .csv\n","\n","        **Example Questions:**\n","        - \"Give me a summary of this data\"\n","        - \"What are the correlations between columns?\"\n","        - \"Which columns have missing data?\"\n","        - \"Show me the distribution of [column name]\"\n","        - \"What trends can you identify?\"\n","\n","        **CSV Troubleshooting:** If your CSV doesn't load properly, make sure it's saved with UTF-8 encoding and uses comma separators.\n","        \"\"\")\n","\n","        with gr.Row():\n","            with gr.Column(scale=1):\n","                file_upload = gr.File(\n","                    label=\"üìÅ Upload Excel/CSV File\",\n","                    file_types=[\".xlsx\", \".xls\", \".csv\"]\n","                )\n","\n","                # Add a test CSV button\n","                test_csv_btn = gr.Button(\"üß™ Test with Sample CSV\", variant=\"secondary\", size=\"sm\")\n","\n","            with gr.Column(scale=2):\n","                query_input = gr.Textbox(\n","                    label=\"‚ùì Ask a question about your data\",\n","                    placeholder=\"e.g., 'What are the main patterns in this data?'\",\n","                    lines=3\n","                )\n","\n","        submit_btn = gr.Button(\"üîç Analyze Data\", variant=\"primary\", size=\"lg\")\n","\n","        output = gr.Textbox(\n","            label=\"üìã Analysis Results\",\n","            lines=15,\n","            max_lines=30\n","        )\n","\n","        # Function to create and test with sample CSV\n","        def create_test_csv():\n","            import tempfile\n","            import os\n","\n","            # Create sample data\n","            sample_data = {\n","                'Product': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Tablet'],\n","                'Price': [999.99, 25.50, 75.00, 299.99, 449.99],\n","                'Sales': [150, 500, 300, 120, 200],\n","                'Category': ['Electronics', 'Accessories', 'Accessories', 'Electronics', 'Electronics'],\n","                'Rating': [4.5, 4.2, 4.0, 4.8, 4.3]\n","            }\n","\n","            df = pd.DataFrame(sample_data)\n","\n","            # Create temporary file\n","            temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n","            df.to_csv(temp_file.name, index=False)\n","            temp_file.close()\n","\n","            return temp_file.name\n","\n","        def test_with_sample_csv():\n","            try:\n","                csv_file = create_test_csv()\n","                result = chatbot.load_excel_file(csv_file)\n","\n","                # Clean up\n","                import os\n","                os.unlink(csv_file)\n","\n","                return result + \"\\n\\nüéØ **Sample loaded successfully!** You can now test queries like 'What is the average price?' or 'Show correlation between price and sales'.\"\n","            except Exception as e:\n","                return f\"Error creating sample CSV: {str(e)}\"\n","\n","        # Examples\n","        gr.Examples(\n","            examples=[\n","                [None, \"Give me a summary of the data\"],\n","                [None, \"What correlations exist between numeric columns?\"],\n","                [None, \"Which columns have missing values?\"],\n","                [None, \"Analyze the distribution of the data\"],\n","                [None, \"What insights can you provide about this dataset?\"],\n","                [None, \"Show me statistics for all numeric columns\"],\n","                [None, \"What are the unique values in categorical columns?\"]\n","            ],\n","            inputs=[file_upload, query_input]\n","        )\n","\n","        submit_btn.click(\n","            fn=process_file_and_query,\n","            inputs=[file_upload, query_input],\n","            outputs=output\n","        )\n","\n","        test_csv_btn.click(\n","            fn=test_with_sample_csv,\n","            outputs=output\n","        )\n","\n","    return interface\n","\n","# Launch the interface\n","if __name__ == \"__main__\":\n","    interface = create_interface()\n","    interface.launch(share=True, debug=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"BBbMiap9Ksap","executionInfo":{"status":"ok","timestamp":1749818673565,"user_tz":-330,"elapsed":737502,"user":{"displayName":"Mayur Gholap","userId":"11537871301684619206"}},"outputId":"2326e185-0fc6-49d9-b2dd-267b3dba1441"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n","Requirement already satisfied: xlrd in /usr/local/lib/python3.11/dist-packages (2.0.1)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.32.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n","Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n","Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n","Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n","Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n","Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.5)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n","Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.12)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n","Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","Loading language model...\n","‚úÖ Model gpt2 loaded successfully!\n","Loading sentence transformer...\n","Chatbot initialized successfully!\n","Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://e001e15c71e35a803d.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://e001e15c71e35a803d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /tmp/gradio/f1ac34adc9959754c44f14f901416071eef3497736a176c94d0d91441ea885ca/sales_data.csv, Extension: csv\n","üîÑ Attempting to load CSV file...\n","Trying config 1: {'encoding': 'utf-8', 'sep': ','}\n","‚úÖ CSV loaded successfully with config 1: (4, 13)\n","üîÑ Cleaning data...\n","üìä Final dataset shape: (4, 13)\n","üìã Columns: ['1', '2024-01-01', 'Alice Shah', 'alice@example.com', '9876543210', 'Laptop', 'Electronics', '1.1', '60000.0', '60000.0.1', 'Credit Card', 'Mumbai, MH', 'Delivered']\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /tmp/gradio/e1dfc3e1f9f864339749d73d86f43c45bde46dbe35f2c47385415bc9bcab4f0d/sales_data_sample copy.csv, Extension: csv\n","üîÑ Attempting to load CSV file...\n","Trying config 1: {'encoding': 'utf-8', 'sep': ','}\n","Config 1 failed: 'utf-8' codec can't decode byte 0x84 in position 8...\n","Trying config 2: {'encoding': 'utf-8', 'sep': ';'}\n","Config 2 failed: 'utf-8' codec can't decode byte 0x84 in position 1...\n","Trying config 3: {'encoding': 'latin-1', 'sep': ','}\n","‚úÖ CSV loaded successfully with config 3: (2823, 25)\n","üîÑ Cleaning data...\n","üìä Final dataset shape: (2823, 25)\n","üìã Columns: ['ORDERNUMBER', 'QUANTITYORDERED', 'PRICEEACH', 'ORDERLINENUMBER', 'SALES', 'ORDERDATE', 'STATUS', 'QTR_ID', 'MONTH_ID', 'YEAR_ID', 'PRODUCTLINE', 'MSRP', 'PRODUCTCODE', 'CUSTOMERNAME', 'PHONE', 'ADDRESSLINE1', 'ADDRESSLINE2', 'CITY', 'STATE', 'POSTALCODE', 'COUNTRY', 'TERRITORY', 'CONTACTLASTNAME', 'CONTACTFIRSTNAME', 'DEALSIZE']\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /tmp/gradio/e1dfc3e1f9f864339749d73d86f43c45bde46dbe35f2c47385415bc9bcab4f0d/sales_data_sample copy.csv, Extension: csv\n","üîÑ Attempting to load CSV file...\n","Trying config 1: {'encoding': 'utf-8', 'sep': ','}\n","Config 1 failed: 'utf-8' codec can't decode byte 0x84 in position 8...\n","Trying config 2: {'encoding': 'utf-8', 'sep': ';'}\n","Config 2 failed: 'utf-8' codec can't decode byte 0x84 in position 1...\n","Trying config 3: {'encoding': 'latin-1', 'sep': ','}\n","‚úÖ CSV loaded successfully with config 3: (2823, 25)\n","üîÑ Cleaning data...\n","üìä Final dataset shape: (2823, 25)\n","üìã Columns: ['ORDERNUMBER', 'QUANTITYORDERED', 'PRICEEACH', 'ORDERLINENUMBER', 'SALES', 'ORDERDATE', 'STATUS', 'QTR_ID', 'MONTH_ID', 'YEAR_ID', 'PRODUCTLINE', 'MSRP', 'PRODUCTCODE', 'CUSTOMERNAME', 'PHONE', 'ADDRESSLINE1', 'ADDRESSLINE2', 'CITY', 'STATE', 'POSTALCODE', 'COUNTRY', 'TERRITORY', 'CONTACTLASTNAME', 'CONTACTFIRSTNAME', 'DEALSIZE']\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /tmp/gradio/e1dfc3e1f9f864339749d73d86f43c45bde46dbe35f2c47385415bc9bcab4f0d/sales_data_sample copy.csv, Extension: csv\n","üîÑ Attempting to load CSV file...\n","Trying config 1: {'encoding': 'utf-8', 'sep': ','}\n","Config 1 failed: 'utf-8' codec can't decode byte 0x84 in position 8...\n","Trying config 2: {'encoding': 'utf-8', 'sep': ';'}\n","Config 2 failed: 'utf-8' codec can't decode byte 0x84 in position 1...\n","Trying config 3: {'encoding': 'latin-1', 'sep': ','}\n","‚úÖ CSV loaded successfully with config 3: (2823, 25)\n","üîÑ Cleaning data...\n","üìä Final dataset shape: (2823, 25)\n","üìã Columns: ['ORDERNUMBER', 'QUANTITYORDERED', 'PRICEEACH', 'ORDERLINENUMBER', 'SALES', 'ORDERDATE', 'STATUS', 'QTR_ID', 'MONTH_ID', 'YEAR_ID', 'PRODUCTLINE', 'MSRP', 'PRODUCTCODE', 'CUSTOMERNAME', 'PHONE', 'ADDRESSLINE1', 'ADDRESSLINE2', 'CITY', 'STATE', 'POSTALCODE', 'COUNTRY', 'TERRITORY', 'CONTACTLASTNAME', 'CONTACTFIRSTNAME', 'DEALSIZE']\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Processing file: /tmp/gradio/e1dfc3e1f9f864339749d73d86f43c45bde46dbe35f2c47385415bc9bcab4f0d/sales_data_sample copy.csv, Extension: csv\n","üîÑ Attempting to load CSV file...\n","Trying config 1: {'encoding': 'utf-8', 'sep': ','}\n","Config 1 failed: 'utf-8' codec can't decode byte 0x84 in position 8...\n","Trying config 2: {'encoding': 'utf-8', 'sep': ';'}\n","Config 2 failed: 'utf-8' codec can't decode byte 0x84 in position 1...\n","Trying config 3: {'encoding': 'latin-1', 'sep': ','}\n","‚úÖ CSV loaded successfully with config 3: (2823, 25)\n","üîÑ Cleaning data...\n","üìä Final dataset shape: (2823, 25)\n","üìã Columns: ['ORDERNUMBER', 'QUANTITYORDERED', 'PRICEEACH', 'ORDERLINENUMBER', 'SALES', 'ORDERDATE', 'STATUS', 'QTR_ID', 'MONTH_ID', 'YEAR_ID', 'PRODUCTLINE', 'MSRP', 'PRODUCTCODE', 'CUSTOMERNAME', 'PHONE', 'ADDRESSLINE1', 'ADDRESSLINE2', 'CITY', 'STATE', 'POSTALCODE', 'COUNTRY', 'TERRITORY', 'CONTACTLASTNAME', 'CONTACTFIRSTNAME', 'DEALSIZE']\n","Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://e001e15c71e35a803d.gradio.live\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Gbe6qPmjOtyf"},"execution_count":null,"outputs":[]}]}